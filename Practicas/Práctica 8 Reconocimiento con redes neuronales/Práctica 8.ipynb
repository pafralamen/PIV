{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 8. Reconocimiento con redes neuronales\n",
    "\n",
    "En esta práctica trabajaremos con redes neuronales y la base de datos MNIST (Modified National Institute of Standards and Technology database). Esta base de datos está compuesta de 60.000 ejemplos de entrenamiento de dígitos escritos a mano, y 10.000 ejemplos para test. Las imágenes utilizadas están redimensionadas, en escala de grises y centradas en cuadrados de 28x28 pixeles. En la siguiente imagen podemos ver un ejemplo de un digito almacenado en la base de datos.\n",
    "![MNIST](./ejemploMNIST.PNG)\n",
    "\n",
    "Una red neuronal está compuesta de neuronas artificiales que están basadas en las biológicas.\n",
    "![Neurona Biológica](./redneuronalbiologica.PNG)\n",
    "\n",
    "La neuronal artificial se modela de la siguiente forma:\n",
    "![Neurona artificial 1](./neuronalartificial1.png)\n",
    "![Neurona artificial 2](./neuronalartificial2.PNG)\n",
    "Como puede verse, en esta red neuronal hay una serie de parámetros que hay que determinar, principalmente:\n",
    "* Los pesos (los w).\n",
    "* Y la función de transferencia.\n",
    "\n",
    "Con una neurona, podemos hacer una separación lineal del espacio tal y como se ven la siguiente figura:\n",
    "![Clasificación](./clasificaciónneurona.png)\n",
    "\n",
    "Para una tarea de aprendizaje normalmente no se trabaja con una sola neurona, sino que se utiliza una red neuronal similar a la que se puede ver en la siguiente figura:\n",
    "![Clasificación](./Colored_neural_network.png)\n",
    "\n",
    "La organización más común de una red neuronal es con múltiples capas, en donde las neuronas de una capa están conectadas únicamente con las neuronas de la capa anterior y con las neuronas de la capa siguiente. La primera capa de neuronas son las neuronas que reciben los datos de entrada y esta capa se denomina la capa de entrada. Las neuronas que están en la última capa son la capa de salida, y es donde nos encontramos el resultado de la red neuronal. Entre estas dos capas nos podemos encontrar cero o más capas ocultas de neuronas.\n",
    "\n",
    "El aprendizaje en una red neuronal consiste en ir ajustando los pesos de la red para ir mejorando la precisión de la red neuronal. Este entrenamiento se realiza observando los errores cometido por la red neuronal con el conjunto de entrenamiento. El algoritmo más habitual para el ajuste de estos pesos es el Backpropagation.\n",
    "\n",
    "En este entrenamiento es importante que determinemos los siguientes parámetros:\n",
    "* Ratio de aprendizaje: la ratio de aprendizaje determina el tamaño de los cambios que se realizarán en los pesos para suavizar los errores de la red neuronal. Un tamaño alto, disminuirá el tiempo de entrenamiento necesario, pero reducirá la precisión final. Un tamaño bajo, aumentará el tiempo de entrenamiento y mejorará la precisión final alcanzada.\n",
    "* epochs: número de veces con el que se entrena con todo el conjunto de entrenamiento. Por ejemplo, epoch = 10 indicaría que se utilizaría 10 veces el conjunto de entrenamiento para entrenar los pesos de la red neuronal\n",
    "\n",
    "Para estudiar la bondad de la clasificación realiza por la red neuronal nos vamos a fijar en tres parámetros:\n",
    "* Precisión con el conjunto de entrenamiento.\n",
    "* Precisión con el conjunto de test.\n",
    "* Matriz de confusión: esta matriz nos indica los errores cometidos. Las filas serían la clase real del ejemplo, mientras que en las columnas tendríamos la clase asignada en la clasificación (ver siguiente figura).\n",
    "![Clasificación](./matrizconfusion.png)\n",
    "\n",
    "## Cuestionario\n",
    "\n",
    "1. Vamos a realizar un estudio de cuales son los parámetros óptimos para la clasificación de los dígitos de la base de datos MNIST. Como hemos comentado anteriormente por un lado tenemos la configuración de la red neuronal, y por otro lado, tenemos los parámetros relacionados con el proceso de aprendizaje. Nuestro objetivo será jugar con dichos valores hasta encontrar una configuración óptima. \n",
    "\n",
    "Para realizar las distintas experimentaciones modificaremos tanto los parámetros que definen la estructura de la red neuronal (número de neuronas en la capa oculta), como los parámetros del aprendizaje (epochs (5, 10, 30) y ratio de aprendizaje (0.01, 0.02, 0.05) construyendo una tabla similar a la siguiente por cada estructura de red neuronal distinta (empezaremos con 4 neuronas en la capa oculta, hasta 28, yendo de 4 en 4). A continuación, vemos las tablas que tienen que obtenerse cuando el número de neuronas en la capa oculta son 4:\n",
    "\n",
    "**Número de neuronas en la capa oculta = 4:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |   |   |   |\n",
    "| **10** |   |   |   |\n",
    "| **30** |   |   |   |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |   |   |   |\n",
    "| **10** |   |   |   |\n",
    "| **30** |   |   |   |\n",
    "\n",
    "Una vez encontrada esa configuración optima, es decir aquella configuración con el mejor valor de acierto sobre el conjunto de test, obtendremos su matriz de confusión (ver tabla siguiente) y analizaremos los resultados.\n",
    "\n",
    "|  | 0 | 1  | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| **0** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **1** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **2** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **3** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **4** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **5** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **6** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **7** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **8** |   |   |   |   |   |   |   |   |   |   |\n",
    "| **9** |   |   |   |   |   |   |   |   |   |   |\n",
    "\n",
    "En ese análisis, **entre otras cuestiones** debería responder a las siguientes preguntas: ¿Qué porcentaje de acierto me da un valor más fiable, el de entrenamiento o el test? ¿Existe algún dígito que presenta una dificultad mayor a la hora de reconocerlo?\n",
    "\n",
    "Para probar las distintas configuraciones modificaré las siguientes líneas de código:\n",
    "```python\n",
    "epochs = 10\n",
    "\n",
    "ANN = NeuralNetwork(network_structure=[image_pixels, 20, 10],\n",
    "                               learning_rate=0.01,\n",
    "                               bias=None)\n",
    "```\n",
    "\n",
    "En estas líneas se establecen por un lado los epochs (=10), la estructura de la red neuronal (20 neuronas en la capa oculta), y la ratio de aprendizaje (learning_rate=0.01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd,\n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "        \n",
    "    \n",
    "    def __init__(self, \n",
    "                 network_structure, # ie. [input_nodes, hidden1_nodes, ... , hidden_n_nodes, output_nodes]\n",
    "                 learning_rate,\n",
    "                 bias=None\n",
    "                ):  \n",
    "\n",
    "        self.structure = network_structure\n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.create_weight_matrices()\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "        self.weights_matrices = []    \n",
    "        layer_index = 1\n",
    "        no_of_layers = len(self.structure)\n",
    "        while layer_index < no_of_layers:\n",
    "            nodes_in = self.structure[layer_index-1]\n",
    "            nodes_out = self.structure[layer_index]\n",
    "            n = (nodes_in + bias_node) * nodes_out\n",
    "            rad = 1 / np.sqrt(nodes_in)\n",
    "            X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + bias_node))\n",
    "            self.weights_matrices.append(wm)\n",
    "            layer_index += 1\n",
    "\n",
    "        \n",
    "        \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "                                       \n",
    "        no_of_layers = len(self.structure)        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "\n",
    "        layer_index = 0\n",
    "        # The output/input vectors of the various layers:\n",
    "        res_vectors = [input_vector]          \n",
    "        while layer_index < no_of_layers - 1:\n",
    "            in_vector = res_vectors[-1]\n",
    "            if self.bias:\n",
    "                # adding bias node to the end of the 'input'_vector\n",
    "                in_vector = np.concatenate( (in_vector, \n",
    "                                             [[self.bias]]) )\n",
    "                res_vectors[-1] = in_vector\n",
    "            x = np.dot(self.weights_matrices[layer_index], in_vector)\n",
    "            out_vector = activation_function(x)\n",
    "            res_vectors.append(out_vector)   \n",
    "            layer_index += 1\n",
    "        \n",
    "        layer_index = no_of_layers - 1\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "         # The input vectors to the various layers\n",
    "        output_errors = target_vector - out_vector  \n",
    "        while layer_index > 0:\n",
    "            out_vector = res_vectors[layer_index]\n",
    "            in_vector = res_vectors[layer_index-1]\n",
    "\n",
    "            if self.bias and not layer_index==(no_of_layers-1):\n",
    "                out_vector = out_vector[:-1,:].copy()\n",
    "\n",
    "            tmp = output_errors * out_vector * (1.0 - out_vector)     \n",
    "            tmp = np.dot(tmp, in_vector.T)\n",
    "            \n",
    "            #if self.bias:\n",
    "            #    tmp = tmp[:-1,:] \n",
    "                \n",
    "            self.weights_matrices[layer_index-1] += self.learning_rate * tmp\n",
    "            \n",
    "            output_errors = np.dot(self.weights_matrices[layer_index-1].T, \n",
    "                                   output_errors)\n",
    "            if self.bias:\n",
    "                output_errors = output_errors[:-1,:]\n",
    "            layer_index -= 1\n",
    "            \n",
    "\n",
    "       \n",
    "\n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              intermediate_results=False):\n",
    "        intermediate_weights = []\n",
    "        for epoch in range(epochs):  \n",
    "            for i in range(len(data_array)):\n",
    "                self.train_single(data_array[i], labels_one_hot_array[i])\n",
    "            if intermediate_results:\n",
    "                intermediate_weights.append((self.wih.copy(), \n",
    "                                             self.who.copy()))\n",
    "        return intermediate_weights      \n",
    "        \n",
    "\n",
    "               \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "\n",
    "        no_of_layers = len(self.structure)\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the inpuy_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        in_vector = np.array(input_vector, ndmin=2).T\n",
    "\n",
    "        layer_index = 1\n",
    "        # The input vectors to the various layers\n",
    "        while layer_index < no_of_layers:\n",
    "            x = np.dot(self.weights_matrices[layer_index-1], \n",
    "                       in_vector)\n",
    "            out_vector = activation_function(x)\n",
    "            \n",
    "            # input vector for next layer\n",
    "            in_vector = out_vector\n",
    "            if self.bias:\n",
    "                in_vector = np.concatenate( (in_vector, \n",
    "                                             [[self.bias]]) )            \n",
    "            \n",
    "            layer_index += 1\n",
    "  \n",
    "    \n",
    "        return out_vector\n",
    "    \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = np.zeros((10, 10), int)\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            cm[res_max, int(target)] += 1\n",
    "        return cm\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs\n",
    "\n",
    "\n",
    "with open(\"data/pickled_mnist.pkl\", \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "\n",
    "lr = np.arange(10)\n",
    "\n",
    "train_labels_one_hot = (lr==train_labels).astype(float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(float)\n",
    "\n",
    "\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "\n",
    "# Inicializamos los valores a ejecutar (modificacion para varias ejecuciones)\n",
    "\n",
    "capas_ocultas = [4,8,12,16,20,24,28] \n",
    "epochs_list = [5,10,30] \n",
    "learning_rate = [0.01,0.02,0.05] \n",
    "   \n",
    "for i in range(len(capas_ocultas)):\n",
    "    for j in range(len(epochs_list)):\n",
    "            for k in range(len(learning_rate)):\n",
    "                \n",
    "                ANN = NeuralNetwork(network_structure=[image_pixels, capas_ocultas[i], 10],\n",
    "                               learning_rate=learning_rate[k],\n",
    "                               bias=None)\n",
    "                print(f\"-----------EJECUCION------ capas {capas_ocultas[i]} ,epochs {epochs_list[j]} y learning_rate {learning_rate[k]} \")\n",
    "                ANN.train(train_imgs, train_labels_one_hot, epochs=epochs_list[j])\n",
    "\n",
    "\n",
    "                corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
    "                print(\"Porcentaje de acierto en entrenamiento: \", corrects / ( corrects + wrongs))\n",
    "                corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
    "                print(\"Porcentaje de acierto en test:\", corrects / ( corrects + wrongs))\n",
    "\n",
    "                cm = ANN.confusion_matrix(train_imgs, train_labels)\n",
    "                print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Número de neuronas en la capa oculta = 4:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.4339 | 0.5490166666666667  | 0.4804  |\n",
    "| **10** |  0.39315 | 0.32493333333333335  |  0.4836666666666667 |\n",
    "| **30** | 0.21428333333333333  |  0.4861333333333333 | 0.3903333333333333  |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.4311  | 0.548  |  0.4792 |\n",
    "| **10** | 0.3883  |  0.3221 | 0.4759  |\n",
    "| **30** | 0.2138  | 0.4827  | 0.3853  |\n",
    "\n",
    "**Número de neuronas en la capa oculta = 8:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.845 | 0.8776833333333334  | 0.8672166666666666  |\n",
    "| **10** | 0.8922333333333333  | 0.8718  | 0.8772833333333333  |\n",
    "| **30** | 0.8757666666666667  | 0.8814  |  0.88335 |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.8432  | 0.8731  |  0.8687 |\n",
    "| **10** | 0.887  | 0.8645  | 0.8807  |\n",
    "| **30** |  0.8712 | 0.8766  |  0.8667 |\n",
    "\n",
    "**Número de neuronas en la capa oculta = 12:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.8998166666666667  | 0.8744333333333333  |  0.8880166666666667 |\n",
    "| **10** | 0.8998333333333334  | 0.8943  | 0.91695  |\n",
    "| **30** | 0.9133833333333333  | 0.9178  | 0.9229666666666667  |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.898 | 0.8763  | 0.8817  |\n",
    "| **10** | 0.8952  |  0.8959 |  0.9096 |\n",
    "| **30** |  0.9049 | 0.9066  |  0.905 |\n",
    "\n",
    "**Número de neuronas en la capa oculta = 16:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.9061  | 0.9172  | 0.91475  |\n",
    "| **10** | 0.90945  | 0.9164666666666667  |  0.92855 |\n",
    "| **30** |  0.9305166666666667 |  0.9359666666666666 |  0.9323333333333333 |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.9032  | 0.9123  | 0.9125  |\n",
    "| **10** |  0.9048 | 0.909  |  0.9212 |\n",
    "| **30** | 0.9211  |  0.9185 |  0.9172 |\n",
    "\n",
    "**Número de neuronas en la capa oculta = 20:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.9208333333333333  |  0.9276833333333333 | 0.9182833333333333  |\n",
    "| **10** |  0.9304666666666667 |  0.9374 | 0.9391833333333334  |\n",
    "| **30** |  0.9469333333333333 | 0.9423833333333334  | 0.9463833333333334  |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** | 0.9193  | 0.9243  |  0.917 |\n",
    "| **10** | 0.9231  |  0.9307 |  0.9326 |\n",
    "| **30** |  0.9332 |  0.9313 | 0.9305  |\n",
    "\n",
    "**Número de neuronas en la capa oculta = 24:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.92765 | 0.9322666666666667  |  0.9385666666666667 |\n",
    "| **10** |  0.9363 | 0.9463  |  0.9394166666666667 |\n",
    "| **30** |  0.9521333333333334 | 0.9502166666666667  | 0.9504166666666667  |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.927 | 0.9289  | 0.9286  |\n",
    "| **10** |  0.9292 |  0.9377 |  0.9329 |\n",
    "| **30** | 0.9381  | 0.9358  | 0.936  |\n",
    "\n",
    "**Número de neuronas en la capa oculta = 28:**\n",
    "\n",
    "Porcentaje de acierto en entrenamiento:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.9307833333333333 | 0.9464833333333333  |  0.94725 |\n",
    "| **10** | 0.9426333333333333  | 0.94245  | 0.9510166666666666  |\n",
    "| **30** | 0.9491  |  0.9640833333333333 |  0.95805 |\n",
    "\n",
    "Porcentaje de acierto en test:\n",
    "\n",
    "|   | 0.01 | 0.02 | 0.05 |\n",
    "|---|---|---|---|\n",
    "| **5** |  0.9286 | 0.9439  | 0.9397  |\n",
    "| **10** | 0.9356  | 0.9371  |  0.9435 |\n",
    "| **30** |  0.9372 | 0.9485  |  0.9404 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración óptima\n",
    "\n",
    "La mejor configuración es **28 neuronas en la capa oculta, 30 epochs y 0.02 de ratio de aprendizaje**\n",
    "\n",
    "|  | 0 | 1  | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| **0** | 5845  |  1  | 39 |  31  | 18 |  26  | 55 |  12 |  36 |  24|\n",
    "| **1** | 0 |6636 |  18  | 12 |  11  |  9  |  9 |  19  | 57  |  9 |\n",
    "| **2** |   9  | 24| 5704  | 69  | 16|   17 |   7  | 35  | 29  |  3 |\n",
    "| **3** | 3  | 13  | 13 |5758 |   1 |  68  |  0  | 10 |  44 |  58 |\n",
    "| **4** |   6  |  5 |  51   | 5 |5594 |  13  | 14 |  39  | 11 |  69 |\n",
    "| **5** | 12 |  17 |  21 | 111 |   6 | 5201 |  47 |   7  | 64 |  18 |\n",
    "| **6** | 18 |   3  | 22 |   9 |  32 |  42 |5770  |  2 |  16  |  6 |\n",
    "| **7** | 0  | 13 |  25  | 50  | 14 |  3 |   1 | 6083  |  7  | 40 |\n",
    "| **8** | 25  | 19 |  54   |58  |  7 |  21  | 12  | 11 |5557  | 25 |\n",
    "| **9** |   5  | 11 |  11 |  28  |143  | 21  |  3  | 47  | 30| 5697 |\n",
    "\n",
    "MEDIA DE ACIERTOS: **5784.5**\n",
    "\n",
    "MEDIA DE FALLOS: **23.944444444444443**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Por qué es la configuración óptima?**\n",
    "\n",
    "Esta configuracíon es la óptima para este conjunto de datos ya que a más neuronas mejores resultados y a más entrenamientos (epochs) en general mejor.\n",
    "\n",
    "Con el ratio pasa algo curioso ya que aunque debería de mejorar cuando se reduce, algo que si que ocurre en general cuando pasamos de 0.05 a 0.02, cuando lo reducimos a 0.01 obtiene peores resultados que con 0.02 o incluso 0.05.\n",
    "\n",
    "\n",
    "\n",
    "**¿Qué porcentaje de acierto me da un valor más fiable, el de entrenamiento o el test?**\n",
    "\n",
    "El de test ya que el de entrenamiento esta sesgado, debido a que al usar esos datos al entrenar los reconocera de forma más sencilla.\n",
    "\n",
    "**¿Existe algún dígito que presenta una dificultad mayor a la hora de reconocerlo?**\n",
    "\n",
    "El 5 es el menos reconocido (5201 veces) aunque no parece destacable, siendo mas destacable el 1 siendo el más reconocido (6636) y estando más alejado respecto a la media.\n",
    "Tiene el segundo valor más erroneo (111) con el 5 reconociendo el 3.\n",
    "\n",
    "El valor más erroneo es (143) con el 9 reconociendo el 4.\n",
    "\n",
    "Si nos fijamos a inversa el 4 con el 9 (69) o el 3 con el 5 (68) nos fijamos en que también destacan frente a la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDIA DE ACIERTOS: 5784.5\n",
      "MEDIA DE FALLOS: 23.944444444444443\n"
     ]
    }
   ],
   "source": [
    "# Programa para hacer la media de fallos / aciertos\n",
    "\n",
    "m = [[5845   , 1  , 39  , 31 ,  18 , 26  , 55  , 12   ,36   ,24],\n",
    " [   0 ,6636 ,  18  , 12  , 11  ,  9 ,   9 ,  19 ,  57 ,  9],\n",
    " [   9  , 24 ,5704  , 69  , 16 ,  17  ,  7 ,  35  , 29  ,  3],\n",
    " [   3   ,13  , 13, 5758  ,  1  , 68 ,   0  , 10 ,  44 ,  58],\n",
    " [   6    ,5 ,  51  ,  5 ,5594  , 13 ,  14  , 39  , 11 ,  69],\n",
    " [  12   ,17 ,  21,  111  ,  6, 5201 ,  47 ,   7  , 64 ,  18],\n",
    " [  18   , 3  , 22  ,  9 ,  32 ,  42, 5770  ,  2  , 16   , 6],\n",
    " [   0  , 13 ,  25,   50  , 14 ,   3  ,  1, 6083 ,   7 ,  40],\n",
    " [  25  , 19  , 54  , 58  ,  7  , 21 ,  12 ,  11 ,5557  , 25],\n",
    " [   5  , 11 ,  11  , 28  ,143  , 21   , 3 ,  47 ,  30, 5697] ]\n",
    "\n",
    "l = len(m[0])\n",
    "mediaAciertos = 0\n",
    "mediaFallos = 0\n",
    "\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        if (i == j):\n",
    "            mediaAciertos += m[i][j]\n",
    "        else:\n",
    "            mediaFallos += m[i][j]\n",
    "            \n",
    "print(f\"MEDIA DE ACIERTOS: {mediaAciertos/l}\") \n",
    "print (f\"MEDIA DE FALLOS: {mediaFallos/(l*(l-1))}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
